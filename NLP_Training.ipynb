{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Word Tokenization"
      ],
      "metadata": {
        "id": "feZdPwDD4qwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python's split() function"
      ],
      "metadata": {
        "id": "GCkO-EFO5hc6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXc7OZ4q4cz1"
      },
      "outputs": [],
      "source": [
        "text = \"My name is Sonali\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9rTFDmS4jZj",
        "outputId": "a4f87254-00e7-4e01-c38a-66f8262ef3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My', 'name', 'is', 'Sonali']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Tokenization\n"
      ],
      "metadata": {
        "id": "ooPcsmdv4w7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"My name is Sonali. I am NLP Lead at Codebugged AI\""
      ],
      "metadata": {
        "id": "h36dBeXP4lYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.split('. ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mHoMFvL46uc",
        "outputId": "de7e2fd2-6c69-4a93-d1e6-82f29a1c12d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My name is Sonali', 'I am NLP Lead at Codebugged AI']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RegEx\n"
      ],
      "metadata": {
        "id": "_4493H4j5n-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "bg-Qijac6gzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"How are you doing? I'm doing fine. How about you?\""
      ],
      "metadata": {
        "id": "CdWslp6h5cXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = re.compile('[.!?] ').split(paragraph)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE1g5QuQ5AUc",
        "outputId": "cb07e292-36fc-4d4b-f62c-422064e14186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How are you doing', \"I'm doing fine\", 'How about you?']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = re.findall(\"[\\w']+\", paragraph)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlqOo1q-6e-J",
        "outputId": "53fb3e04-37b3-484e-c2fe-573321e8a800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How', 'are', 'you', 'doing', \"I'm\", 'doing', 'fine', 'How', 'about', 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using NLTK"
      ],
      "metadata": {
        "id": "e2Lb8e9E7DC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "vOVNTHkb7SQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "aYR21eT260sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhZQ8yNG7Pma",
        "outputId": "7a20170a-4e62-4ed2-9534-965dfecfe2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhJD7c0c7AxD",
        "outputId": "90fa3d40-6468-4b7c-d195-7a44ce5af86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How',\n",
              " 'are',\n",
              " 'you',\n",
              " 'doing',\n",
              " '?',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'doing',\n",
              " 'fine',\n",
              " '.',\n",
              " 'How',\n",
              " 'about',\n",
              " 'you',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "d7bMoEFg7Jwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUbqVgNZ7vep",
        "outputId": "0e8f36f5-5fab-41e8-ce58-8e9f8716970b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How are you doing?', \"I'm doing fine.\", 'How about you?']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Porter Stemmer"
      ],
      "metadata": {
        "id": "4cYMgDo--G76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "dv5kSJzw7yXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "words = ['Waits', 'wait', 'waiting', 'waited']\n",
        "for word in words:\n",
        "    print(word,\"--->\",porter.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF-QkNqB-J5g",
        "outputId": "1a1d0682-2604-4ab4-b504-e3b5b845e55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waits ---> wait\n",
            "wait ---> wait\n",
            "waiting ---> wait\n",
            "waited ---> wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "TgQsxARY-qxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer(language='english')\n",
        "words = ['Waits', 'wait', 'waiting', 'waited']\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJSjeyxR_IHx",
        "outputId": "d64f04ec-15f3-40a2-b744-847e6d5ece3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waits ---> wait\n",
            "wait ---> wait\n",
            "waiting ---> wait\n",
            "waited ---> wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob Lemmatiztaion"
      ],
      "metadata": {
        "id": "WUb0CyDdDUjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-K4lt-NAZ14",
        "outputId": "b319c090-bbbf-47d4-ccdb-bc97543c2658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listofwords = ['waiting', 'plays', 'flies', 'borrowed']\n",
        "for words in listofwords:\n",
        "    w = Word(words)\n",
        "    print(words + \" ---> \" + w.lemmatize())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8aa3mKjFMHl",
        "outputId": "b59ebf85-cf9b-4404-b514-762d8ea6ae03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting ---> waiting\n",
            "plays ---> play\n",
            "flies ---> fly\n",
            "borrowed ---> borrowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Word('relations').lemmatize())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og8DHo1BDZZq",
        "outputId": "6b1ae77e-a21e-492b-c5b3-e5b4d418295b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet Lemmatizer"
      ],
      "metadata": {
        "id": "vFL253UHE1Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        " \n",
        "wnl = WordNetLemmatizer()\n",
        "list1 = ['waiting', 'plays', 'flies', 'borrowed']\n",
        "for words in list1:\n",
        "    print(words + \" ---> \" + wnl.lemmatize(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxqT7IRqDgBl",
        "outputId": "5004b131-57f2-472a-852c-58f5d1b13e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting ---> waiting\n",
            "plays ---> play\n",
            "flies ---> fly\n",
            "borrowed ---> borrowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy Lemmatization"
      ],
      "metadata": {
        "id": "XEe8Pbn4GkNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "doc = nlp(u\"how many times you ate today\")\n",
        "for token in doc:\n",
        "    print(token, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CgouGfRFFOo",
        "outputId": "b7e6e173-6b53-47f6-9a29-df627e3e6b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how how\n",
            "many many\n",
            "times time\n",
            "you -PRON-\n",
            "ate eat\n",
            "today today\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarity"
      ],
      "metadata": {
        "id": "j9Jy-FjVA7bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "\n",
        "nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "r4_ffosx9XPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_1 = nlp(\"how can I check my account balance\")\n",
        "\n",
        "sent_2 = nlp(\"I want to check my account balance.\")\n",
        "\n",
        "print(sent_2.similarity(sent_1))"
      ],
      "metadata": {
        "id": "SfSgj90GGsiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8caa3e16-573e-4000-a19a-75af21a486b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7669168537352331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stopwords"
      ],
      "metadata": {
        "id": "JPLz3XgYA-zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TExXToe9-jR",
        "outputId": "0fe54676-f7d0-4202-bc80-42df80dd0674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBcrbqrZ8wZ-",
        "outputId": "544b04fe-12bf-447a-b916-f50b96e7ae50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens_1 = word_tokenize(\"How can I check my account balance\")\n",
        "new_sent_1 = [w for w in word_tokens_1 if not w.lower() in stop_words]\n",
        "new_sent_1= ' '.join([str(elem) for elem in new_sent_1])\n",
        "print(new_sent_1)\n",
        "\n",
        "word_tokens_2 = word_tokenize(\"I want to check my account balance\")\n",
        "new_sent_2 = [w for w in word_tokens_2 if not w.lower() in stop_words]\n",
        "new_sent_2= ' '.join([str(elem) for elem in new_sent_2])\n",
        "print(new_sent_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAI9Cfkg99lG",
        "outputId": "f84569ed-0176-40a9-c19a-8c969d84b64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check account balance\n",
            "want check account balance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent_1= nlp(new_sent_1)\n",
        "new_sent_2 = nlp(new_sent_2)\n",
        "print(new_sent_2.similarity(new_sent_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhXvjXZW96Zn",
        "outputId": "ead61783-bdae-4a0c-9c50-871bb3568edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dvbADwpsAF9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}